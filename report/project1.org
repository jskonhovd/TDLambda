* Project 1 : Sutton 1988

* Instructions
Result Replication Presentation
Note: Submission will be on the RLDM website like the last homeworks.

One aspect of research in reinforcement learning (or any scientific
field) is the replication of previously published results. One benefit
to replication is to aid your own understanding of the
results. Another is that it puts you in a good position for being able
to extend and compare new contributions to what’s in the existing
literature. Replication can be challenging. Researchers often find
that important parameters needed to replicate results from papers are
not stated in papers or that the procedures stated in papers have
ambiguity or subtle errors. Sometimes obtaining the same pattern of
results isn’t possible.  

Read Richard Sutton’s 1988 paper and create an implementation and
replication of the results in Figures 3, 4, and 5. (You might also
compare these results with those in Chapter 7 of Sutton’s textbook:
https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node73.html.) 

You will present your work via a short (at most 3 minutes) video
presentation and a 2-to-3-page written report. The report/presentation
should include a description of the experiment replicated, how the
experiment was implemented, and the outcome of the experiment. You
should describe how well the results match the results given in the
paper as well as significant differences. Also describe any pitfalls
you ran into while trying to replicate the experiment from the paper
(e.g. unclear parameters, contradictory descriptions of the procedure
to follow, results that differ wildly from the published
results). What steps did you take to overcome those pitfalls? What
assumptions did you make? Why are these assumptions justified? 

Grades will be based on the fidelity of the replication (25%), how
well you show you understand the original paper (25%), the quality of
your presentation (25%), and your written report (25%). 

You will have to submit it on the RLDM website. Details will come soon.

** How to make video presentations?
Do what you think is the simple. For example, Keynote allows you to
record audio over your presentations. 

** Make a slide set.
Start a screen recorder and record your voice while you present.
Some students might want to use a handy cam. That’s fine too. Just let
us see your slides properly. 

* Instructions: Reproduce figure 3, 4, and 5.

** Random Walk Problem

Our example is one of the simplest of dynamical
systems, that which generates bounded random walks.
A bounded random walk is a state sequence generated by taking random
steps to the right or to the left until a boundary is reached. Figure 2 shows a
system that generates such state sequences. Every walk begins in the center
state D. At each step the walk moves to a neighboring state, either to the right
or to the left with equal probability. If either edge state (A or G) is entered,
tile walk terminates. A typical walk might be DCDEFG. Suppose we wish
to estimate the probabilities of a walk ending in the rightmost state, G, given
that it is in each of the other states. 

We applied linear supervised-learning and TD methods to this problem in
a straightforward way. A walk's outcome was defined to be z = 0 for a walk
ending on the left at A and z = 1 for a walk ending on the right at G.
The learning methods estimated the expected value of z; for this choice of
z, its expected value is equal to the probability of a right-side termination.
For each nonterminal state i, there was a corresponding observation vector
x_i; if the walk was in state i at time t then x_{t} = x_{i}.

Thus; if the walk DCDEFG occurred, then tile learning procedure would
be given the sequence X_{D}, X_{C}, X_{D},X_{E}, X_{R}, 1. The vectors {x_{}_{i}} were the
unit basis vectors of length  5, that is, four of their components
were 0 and the fifth was 1 (e.g., XD = (0, 0, 1, 0, 0)^{T}), with the one
appearing at, a different component for each state.

Thus, if the state the walk was in at time t has its 1 at the ith component
of its observation vector, then the prediction P_{t} = 'w^{T}x_{t} was simply the value
of the ith component of w. We use this particularly simple case to make this
exainple as clear as possible. The theorems we prove later for a more general
class of dynanfical systems require only that the set of observation vectors {xi}
be linearly independent. 


** Figure 3:

Average error on the random-walk problem under repeated presentations.
All data are from TD(A) with different values of A. The dependent measure
used is the RMS error between the ideal predictions and those found by the
learning procedure after being repeatedly presented with the training set
until convergence of the weight vector. This measure was averaged over
100 training sets to produce the data shown. The A = 1 data point is
the performance level attained by the Widrow-Hoff procedure. For each
data point, the standard error is approximately ~ = 0.01, so the differences
between the Widrow-Hoff procedure and the other procedures are highly
significant


** Figure 4 :

Average error on random walk problem after experiencing 10 sequences.
All data are from TD(\lambda) with different values of a and A. The dependent
measure is the RMS error between the ideal predictions and those found
by the learning procedure after a single presentation of a training set.
This measure was averaged over 10O training sets. The \lambda = 1 data points
represent performances of the Widrow-Hoff supervised-learning procedure. 


** Figure 5 :

Average error at best ~ value on random-walk problem. Each data point
represents the average over 100 training sets of the error in the estimates
found by TD()~), for particular A and a values, after a single presentation
of a training set. The ,~ value is given by the horizontal coordinate. The a
value was selected from those shown in Figure 4 to yield the lowest error
for that ), value. 
